{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Linear Regression using `keras`\n",
    "\n",
    "As the [Keras Documentation](http://keras.io/) states, \"Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano\". In that regard, it's not really meant to be used to perform simple linear regression. However, in keeping with the theme of building up from simple linear regression, we're going to implement it using `keras`. \n",
    "\n",
    "## Computational Graphs for Simple Linear Regression \n",
    "\n",
    "As a reference, the computational graphs that we used to visualize the forward and backward propagation steps in solving our simple linear regression problem with gradient descent are as follows: \n",
    "\n",
    "### Forward Propagation \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_forprop.png\" width=400\\>\n",
    "\n",
    "### Backward Propagation \n",
    "\n",
    "<img src=\"../imgs/custom/simp_linear_comp_graph_backprop.png\" width=400\\>\n",
    "\n",
    "### Performing Simple Linear Regression with Keras\n",
    "\n",
    "As noted in it's documentation, `keras` can be run on top of either `tensorflow` or `theano`. This means that under the hood of our simple linear regression using `keras`, a similar version of the code that we wrote in our `theano` or `tensorflow` implementation is being run. By default, `keras` runs on `theano`, but by [adjusting our keras configuration file](http://keras.io/backend/#switching-from-one-backend-to-another), we can easily change that. For now, though, we'll just run it on `theano`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Input\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from datasets.general import gen_simple_linear\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_keras_model(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Specify a placeholder for the inputs. \n",
    "    xs = Input(shape=(1,))\n",
    "    # 2. Define the equation that generates predictions. \n",
    "    ys = Dense(1, activation='linear')(xs)\n",
    "\n",
    "    # 3. Define a `Model` object that will be used to train/learn the coefficients. \n",
    "    linear_model = Model(input=xs, output=ys)\n",
    "    \n",
    "    # 4. Define the optimizer and loss function used to train/learn the coefficients. \n",
    "    sgd = SGD(learning_rate)\n",
    "    \n",
    "    # 5. Compile the model (basically, build up the backpropagation steps)\n",
    "    linear_model.compile(loss='mean_squared_error', optimizer=sgd)\n",
    "    \n",
    "    return linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite the code looking pretty different from our prior implementations in `theano` and `tensorflow`, our `get_keras_model` function returns back what our `get_theano_graph` ([notebook 1c](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/01-simple-linear/1c_nn_th.ipynb)) and `get_tensorflow_graph` ([notebook 1d](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/01-simple-linear/1d_nn_tf.ipynb)) functions returned. It returns a set of computations that perform forward and backward propagation in order to solve a simple linear regression problem using gradient descent.\n",
    "\n",
    "The most notable feature of our `get_keras_model` compared to our prior implementations is it's smaller code base, which makes sense given it's goal to be a \"minimalist, highly modular neural networks library\". Our forward propagation is defined in 2 steps, compared to the 5 steps it took with `theano` or `tensorflow`: \n",
    "\n",
    "* Step `1` is the `keras` way of generating a placeholder variable that will later be replaced with real data. The one piece of information we have to provide is the dimensionality of one of our input observations (e.g. how many features it has). With simple linear regression, we only have one feature. \n",
    "* Step `2` defines our linear regression equation, <img src=\"../imgs/equations/simp_linear.png\" width=100 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\>. We'll get more into the details of how this `Dense` class works at a later point when we cover some of the terminology of neural networks. For now, just trust that it defines our simple linear regression equation. \n",
    "\n",
    "Our backward propagation step is defined in steps `4-5`: \n",
    "\n",
    "* Step `4` specifies exactly how to perform our gradient descent updates. Here, we'll use what we've used in all of our prior implementations - gradient descent with a learning rate of 0.1. As we'll see in later notebooks, there are a number of more complicated flavors of gradient descent that we also have the option to use.\n",
    "* Step `5` tells `keras` to calculate the update rules for our coefficients, defining each of the steps necessary for doing so (including the calculating of all necessary derivatives). Here, we have to specify a `loss` as well as an `optimizer`. \n",
    " The `optimizer` is what we discussed directly above - it specifies how to perform our gradient descent updates. The `loss` function specifies how we calculate the error, which up to this point has been using squared error. After defining both a `loss` and `optimizer`, `keras` has all of the pieces it needs to calculate the update rules for our coefficients, and to add those update steps into the graph that it will later run through. \n",
    "\n",
    "Step `3` builds a model object that we can later use to learn our coefficients. To instantiate it, we have to specify `input` as well as `output`. To finish building it and make it usable, we have to run `compile` on it like we did in step `5`. \n",
    "\n",
    "Let's now use our `keras` model to actually solve a linear regression problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 9.167618053584903e-12\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZkAAAEZCAYAAABFFVgWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNTRzTHscAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11043f358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate a beta_0, beta_1, and number of observations, used to generate \n",
    "# fake data to fit. We need a minimum of 2 obs. \n",
    "true_beta_0, true_beta_1 = np.random.randint(2, 10, size=2) \n",
    "n_obs = np.random.randint(9500, 10500)\n",
    "\n",
    "# Generate the data that follows a linear relationship specified \n",
    "# by true_beta_0 and true_beta_1.\n",
    "xs, ys = gen_simple_linear(true_beta_0, true_beta_1, n_obs)\n",
    "\n",
    "# Generate the keras model and print out the initial weights.\n",
    "linear_model = get_keras_model()\n",
    "init_weights = linear_model.get_weights()\n",
    "init_beta_0, init_beta_1 = init_weights[1][0], init_weights[0][0][0]\n",
    "\n",
    "# Learn the coefficients (perform iterations of forward and backward propagation)\n",
    "linear_model.fit(xs, ys, nb_epoch=5000, verbose=0, batch_size=n_obs)\n",
    "# The history attribute holds a history dictionary that we can use to access the\n",
    "# loss (mean squared error) after each iteration. \n",
    "mean_squared_errors = linear_model.history.history['loss']\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(mean_squared_errors, iterations=(100, 5000))\n",
    "print(\"Final Error: {}\".format(mean_squared_errors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run our `keras` model and learn the coefficients, we simply call `fit` on it, making sure to pass in our inputs and outputs (`xs` and `ys`). Here, we also use the `nb_epoch` argument to specify how many iterations of forward and backward propagation to perform over the entire data set (the default value for `nb_epoch` is 10), and `batch_size` to control how many observations the model looks at in each individual foward/backward propagation step (right now we want it to just look at all of them). We'll detail these parameters more later as we dive into neural networks.\n",
    "\n",
    "Upon running our model, we can see that we are also able to solve our simple linear regression problem using `keras`, and that we again are able to obtain effectively zero mean squared error. \n",
    "\n",
    "We'll now move on to looking at multiple linear regression from a neural network perspective. In viewing it as a computational graph as well and coding it up using `numpy`, `theano`, `tensorflow`, and `keras`, we'll move one step closer to discussing true neural networks. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
