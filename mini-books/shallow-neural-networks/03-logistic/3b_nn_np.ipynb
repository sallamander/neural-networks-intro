{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Using Gradient Descent\n",
    "\n",
    "In the previous notebook, we looked at a high level overview of using gradient descent to solve for the beta coefficients in logistic regression. Now, we'll formalize the gradient descent procedure a little bit more, walk through the calculation of the needed derivatives, and code it all up using `numpy`. \n",
    "\n",
    "## Gradient Descent for Logistic Regression \n",
    "\n",
    "Before diving into gradient descent, we'll introduce one new piece of notation (this will help with the derivative calculations below) - we're going to denote the weighted sum of the inputs (<img src=\"../imgs/variables/x_beta.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30 \\>) as <img src=\"../imgs/variables/z.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=17 \\>. This is common notation for the **weighted sum of the inputs to a node**, and it will come into play with the derivative calculations in step 2C below. \n",
    "\n",
    "### Gradient Descent Procedure \n",
    "\n",
    "With gradient descent, we'll do the following: \n",
    "\n",
    "1. Randomly initialize values for our coefficients: \n",
    "<img src=\"../imgs/variables/beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=20 \\>, \n",
    "<img src=\"../imgs/variables/beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=22 \\> , \n",
    "<img src=\"../imgs/variables/beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=19 \\> , and\n",
    "<img src=\"../imgs/variables/beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=18 \\>. \n",
    "\n",
    "2. While we haven't met some stopping condition:   \n",
    " A. Calculate our predicted values, \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>.  \n",
    " B. Calculate the error for each observation using the true values\n",
    "<img src=\"../imgs/variables/yi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=17 \\>, \n",
    "our predicted values \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=16 \\>, \n",
    "and our error formula: \n",
    "<img src=\"../imgs/equations/ind_bin_crossentropy.png\" width=350 \\>    \n",
    " C. For each observation, calculate the gradient of the error with respect to each one of our coefficients (\n",
    "<img src=\"../imgs/derivatives/ei_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/ei_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/ei_beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/ei_beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30\\>\n",
    "), and then use the average across observations to update the coefficients (\n",
    "<img src=\"../imgs/variables/alpha.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=15\\>\n",
    " is the learning rate): \n",
    "<img src=\"../imgs/updates/beta0_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"../imgs/updates/beta1_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"../imgs/updates/beta2_simp_linear_update.png\" width=150 \\>\n",
    "<img src=\"../imgs/updates/beta3_simp_linear_update.png\" width=150 \\>\n",
    "\n",
    "### Derivative Calculations\n",
    "\n",
    "Before calculating any derivatives, recall that our predicted values, \n",
    "<img src=\"../imgs/variables/yhat.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=13 \\>\n",
    ", are given by our logistic regression equation: \n",
    "<img src=\"../imgs/equations/logistic_activation2.png\" align=\"center\" width=\"200\"\\> \n",
    "Recall also that we are now replacing \n",
    "<img src=\"../imgs/variables/x_beta.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=30 \\> \n",
    "with \n",
    "<img src=\"../imgs/variables/z.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=17 \\>, which means we can denote our logistic regression equation via: \n",
    "\n",
    "<img src=\"../imgs/equations/logistic_activation3.png\" align=\"center\" width=\"175\"\\>\n",
    "\n",
    "To calculate the derivatives for an individual observation, we'll use the chain rule that we looked at last notebook, but we'll also have to factor in the non-linear activation function we're now applying (e.g. the sigmoid activation). To denote this, we'll break apart the derivatives of the output with respect to the coefficients (<img src=\"../imgs/derivatives/yhati_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/yhati_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>\n",
    "<img src=\"../imgs/derivatives/yhati_beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/yhati_beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=35\\>), and denote \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=16 \\>\n",
    "as <img src=\"../imgs/variables/sigma_zi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=45 \\>:\n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain_logistic.png\" width=300\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain_logistic.png\" width=300\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta2_chain_logistic.png\" width=300\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta3_chain_logistic.png\" width=300\\>\n",
    "\n",
    "The first two pieces of each of the above chain rules are what we looked at last notebook, whereas the third (rightmost) piece is where we've broken apart the derivatives of the output with respect to the coefficients. For each of these rightmost pieces, we can break them down even farther into each of the individual pieces - \n",
    "<img src=\"../imgs/derivatives/ei_sigma_zi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=60\\>, \n",
    "<img src=\"../imgs/derivatives/sigma_zi_zi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=50\\>, \n",
    "<img src=\"../imgs/derivatives/zi_beta0.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/zi_beta1.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=30\\>, \n",
    "<img src=\"../imgs/derivatives/zi_beta2.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=32\\>, \n",
    "<img src=\"../imgs/derivatives/zi_beta3.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0\" width=32\\>. We can calculate those as follows: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_sigma_zi_soln.png\" width=350\\> \n",
    "<img src=\"../imgs/derivatives/sigma_zi_zi_soln.png\" width=250\\> \n",
    "<img src=\"../imgs/derivatives/zi_beta0_soln.png\" width=80\\>\n",
    "<img src=\"../imgs/derivatives/zi_beta1_soln.png\" width=100\\>\n",
    "<img src=\"../imgs/derivatives/zi_beta2_soln.png\" width=100\\> \n",
    "<img src=\"../imgs/derivatives/zi_beta3_soln.png\" width=100\\>\n",
    "\n",
    "If we plug these back into the original equations, we can obtain our full updates for step 2C: \n",
    "\n",
    "<img src=\"../imgs/derivatives/ei_beta0_chain_logistic_soln.png\" width=350\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta1_chain_logistic_soln.png\" width=350\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta2_chain_logistic_soln.png\" width=350\\>\n",
    "<img src=\"../imgs/derivatives/ei_beta3_chain_logistic_soln.png\" width=350\\>\n",
    "\n",
    "Note that in the final step of each of the update steps, we have switched from <img src=\"../imgs/variables/sigma_zi.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=45 \\> \n",
    "back to \n",
    "<img src=\"../imgs/variables/yhati.png\" style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" width=16 \\>. This was so that we could see that these are the exact same update steps that we used in solving multiple linear regression with gradient descent! It turns out that when we use a **sigmoid activation function** in combination with **binary crossentropy loss**, we obtain the same update steps as when we use an **identity activation** in combination with **squared error loss**. This means that when we code this up with `numpy` below, the only difference from our `numpy` implementation for multiple linear regression will be the forward pass where we calculate our predicted values. The backward pass will look exactly the same! \n",
    "\n",
    "## Logistic Regression using Gradient Descent with `numpy`\n",
    "\n",
    "To demonstrate using gradient descent to solve logistic regression, we'll use the `gen_multiple_logistic` function from the `datasets/general.py` script to generate some toy data that follows a multivariate logistic relationship with three variables. We'll input a `1d numpy array` of betas as well as a number of observations, and it will output data that follows a multivariate logistic relationship (\n",
    "<img src=\"../imgs/equations/logistic.png\" width=120 style=\"vertical-align: text-middle; display: inline-block; padding-top:0; margin-top:0;\" \\> ). We'll then binarize this data by labeling all those observations greater than 0.5 with a `1`, and all those less than or equal to 0.5 with a `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets.general import gen_multiple_logistic\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn_w_gradient_descent(xs, ys): \n",
    "    learning_rate = 0.1\n",
    "    # Step 1 - randomly initialize values for our coefficients.  \n",
    "    betas_array = np.random.random(size=4)\n",
    "        \n",
    "    binary_crossentropy_lst  = []\n",
    "    for _ in range(50000): \n",
    "        # Step 2A - calculate our predicted values. \n",
    "        yhats = 1 / (1 + np.exp(-xs.dot(betas_array)))\n",
    "        yhats = yhats.reshape(-1, 1) # Force `yhats` to have a second \n",
    "                                     # dimension for later calculations.\n",
    "        \n",
    "        # Step 2B - calculate our errors. \n",
    "        diffs = (ys - yhats)\n",
    "        es = -(ys * np.log(yhats) + (1 - ys) * np.log(1 - yhats))\n",
    "        binary_crossentropy_lst.append(es.mean())\n",
    "        \n",
    "        # Step 2C - calculate the gradient of the error with respect to the coefficients, \n",
    "        # and use it to update the coefficients. \n",
    "        d_betas_array = -diffs * xs\n",
    "        betas_array -= learning_rate * d_betas_array.mean(axis=0)\n",
    "        \n",
    "    return binary_crossentropy_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 0.000200005445155866\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZYAAAEZCAYAAAC0HgObAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxGJmZoVyYjEzs0L9f1+msPFdlFFjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a781240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We should have a minimum of 4 obs., since we have \n",
    "# 4 coefficients. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(9500, 10500) \n",
    "\n",
    "# Generate the data that follows a logistic relationship specified \n",
    "# by true_betas_array.\n",
    "xs, ys = gen_multiple_logistic(true_betas_array, n_obs)\n",
    "ys = ys > 0.5\n",
    "\n",
    "# Learn the coefficients using gradient descent. \n",
    "binary_crossentropy_errors = learn_w_gradient_descent(xs, ys)\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(binary_crossentropy_errors, iterations=(100, 50000))\n",
    "print(\"Final Error: {}\".format(binary_crossentropy_errors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with simple and multiple linear regression, we can in fact solve logistic regression using gradient descent. If we run it multiple times, we can see that it is able to achieve incredibly low error each time (ocassionally we see that more iterations would be helpful, though). \n",
    "\n",
    "Viewing logistic regression as a computational graph, introducing the terminology of an **activation function**, and using a non-linear activation function moves us one step closer to discussing actual neural networks. As we'll see in the final chapter (`4`) of this mini-book, non-linear activation functions are a large part of the reason that neural networks are so powerful. \n",
    "\n",
    "We'll now move on to coding this up using `theano`. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
