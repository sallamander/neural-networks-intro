{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression using `tensorflow`\n",
    "\n",
    "Just as with `theano`, we'll use `tensorflow` to build up a computational graph and take advantage of it's automatic differentiation to learn the coefficients for our multiple linear regression problem. \n",
    "\n",
    "## Computational Graphs for Multiple Linear Regression \n",
    "\n",
    "Let's keep the computational graph visuals around for reference: \n",
    "\n",
    "### Forward Propagation\n",
    "\n",
    "<img src=\"../imgs/custom/mult_linear_comp_graph_condensed_forprop.png\" width=300\\>\n",
    "\n",
    "### Backward Propagation\n",
    "\n",
    "<img src=\"../imgs/custom/mult_linear_comp_graph_condensed_backprop.png\" width=400\\>\n",
    "\n",
    "### Building a computational graph with `tensorflow`\n",
    "\n",
    "The biggest difference between the `numpy` implementation and the `tensorflow` implementation of our multiple linear regression problem is that we'll be able to use the automatic differentiation that `tensorflow` offers. In comparing `theano` to `tensorflow`, we'll see similar syntax when building up the graph, but different syntax when actually performing the iterations of gradient descent. Let's take a look..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datasets.general import gen_multiple_linear\n",
    "from utils.plotting import plot_errors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_tensorflow_graph(): \n",
    "    learning_rate = 0.1\n",
    "    # 1. Define placeholder matrices for inputs.\n",
    "    xs = tf.placeholder(tf.float64, name='xs') \n",
    "    ys = tf.placeholder(tf.float64, name='ys')\n",
    "    # 2. Define randomly initialized floats for our betas. \n",
    "    betas = tf.Variable(np.random.random(size=(4, 1)), name='betas')\n",
    "\n",
    "    # 3. Define the equation that generates predictions.\n",
    "    yhats = tf.matmul(xs, betas)\n",
    "    # 4. Define the equation that generates our errors. \n",
    "    es = 0.5 * (ys - yhats) ** 2\n",
    "    # 5. Define the aggregate cost (mean of squared errors)\n",
    "    E = tf.reduce_mean(es)\n",
    "    # 6. Take advantage of `tensorflows` optimizer to automate differentiation\n",
    "    #    as well as the update step. \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train = optimizer.minimize(E)\n",
    "    \n",
    "    return E, betas, train, xs, ys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our multiple linear regression solution with `tensorflow` is going to differ from our `theano` solution ([notebook 2c](https://github.com/sallamander/neural-networks-intro/blob/master/mini-books/shallow-neural-networks/02-multiple-linear/2c_nn_th.ipynb)) in the same way it did for simple linear regression -  `get_tensorflow_graph` will return back the steps necessary to perform forward and backward propagation as multiple pieces, rather than one callable function. Steps `1-5` will still correspond to the forward pass and step `6` the backward pass. Compared to our `tensorflow` implementation for simple linear regression, we see a slight difference in step `6`, which we'll walk through below. \n",
    "\n",
    "In terms of the individual pieces being returned: \n",
    "\n",
    "1. The mean squared error (`E`) is returned so that we can track it through each iteration. \n",
    "2. `betas` are returned so that we can reference them below to initialize them (see the `tf.intialize_variables` call; we could also use `tf.initialize_all_variables`). Note the generation of `betas` as a [Variable object](https://www.tensorflow.org/versions/r0.9/get_started/basic_usage.html#variables) - this is what allows our coefficient values to be updated and shared across iterations. \n",
    "3. `train` holds the magic of our computational graph, and is different from how we solved simple linear regression with `tensorflow`. With simple linear regression, we were explicit about our backward propagation steps, and coded up each individual equation for updating our coefficients. Here, we instead feed the quantity that we want minimized (the **mean squared error**, `E`) to a [tensorflow Optimizer](https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html), which provides us with a simple interface for performing gradient descent. \n",
    "\n",
    " When we call [minimize](https://www.tensorflow.org/versions/r0.10/api_docs/python/train.html#processing-gradients-before-applying-them) on this `Optimizer`, it builds the calculation of the derivatives and the performing of the updates into our computational graph. It by default takes the derivatives of all of the `tf.Variable` objects that it finds in the computational graph prior to that step. Here, this is just `betas`. \n",
    " \n",
    " When `train` is run in a session below, any steps that are necessary to perform the minimization step will be run, which in effect is every step that is part of the forward and backward propagation.\n",
    "4. Finally, `xs` and `ys` are placeholders for our data, and are returned so that we can tell `tensorflow` exactly what part of our graph the real data should line up with. \n",
    "\n",
    "Next, we'll use one of the `Session` objects we've discussed to perform gradient descent and learn the true values for each beta coefficient in `betas`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Error: 1.375943565849604e-21\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEZCAYAAABSN8jfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzMrzMnEzMwKczIxM7PC/j9Z5EF5Pk0s0wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1178aa710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Randomly generate our betas and number of observations, used to generate \n",
    "# fake data to fit. We should have a minimum of 4 obs, since we have \n",
    "# four coefficients. \n",
    "true_betas_array = np.random.randint(2, 10, size=4)\n",
    "n_obs = np.random.randint(9500, 10500) \n",
    "\n",
    "# Generate the tensorflow graph. This is in a function so that we can run this cell multiple \n",
    "# times and obtain different randomly generated values for our `betas` array. \n",
    "E, betas, train, xs, ys = get_tensorflow_graph()\n",
    "\n",
    "# Generate the data that follows a linear relationship specified by `true_betas_array`.\n",
    "x, y = gen_multiple_linear(true_betas_array, n_obs)\n",
    "\n",
    "# Define the initialization operation. \n",
    "init = tf.initialize_variables([betas])\n",
    "with tf.Session() as sess: \n",
    "    sess.run(init) # Perform the actual initialization operation. \n",
    "\n",
    "    # Perform iterations (forward & backward prop.) over the tensorflow graph\n",
    "    mean_squared_errors = []\n",
    "    for step in range(5000):\n",
    "        mean_squared_error, _ = sess.run([E, train], feed_dict={xs : x, ys : y}) \n",
    "        mean_squared_errors.append(mean_squared_error)\n",
    "# Skip the first 100 values because it pulls the y-axis up quite a bit. \n",
    "plot_errors(mean_squared_errors, iterations=(100, 5000))\n",
    "print(\"Final Error: {}\".format(mean_squared_errors[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with our simple linear regression implementation, we'll run gradient descent for our multiple linear regression problem via a `Session` object. This [Session object](https://www.tensorflow.org/versions/r0.9/api_docs/python/client.html#session-management) allows us to encapsulate all of the calculations and implementation details of our graph (or any graph) into a single environment. When running multiple graphs, each of which might have their own specialized environment, this is incredibly helpful. \n",
    "\n",
    "After initializing a `Session` object, our first step is to [initialize any variables](https://www.tensorflow.org/versions/r0.9/how_tos/variables/index.html#initialization) that are going to be used in the graph. Here, this is just the `betas` variable. Once we have created a `Session` object and initialized all of our variables, we can run parts of our graph or ask for values of variables by passing them into `Session.run`. Any time that we want to view the values held in the `betas` variable, for example, we run:  \n",
    "\n",
    "```\n",
    "Session.run(betas)\n",
    "``` \n",
    "\n",
    "If we run this **before** any iterations of gradient descent have been performed, we'll be given back the initial values given to each of the coefficients in `betas`. In order to run one iteration of the gradient descent procedure, we pass `train` into `Session.run`: \n",
    "\n",
    "```\n",
    "Session.run(train, feed_dict={xs : x, ys : y})\n",
    "```\n",
    "\n",
    "\n",
    "When this piece (or any piece) of the computational graph is passed into `Session.run`, any steps necessary to compute what is asked for will be run. For `train`, this is every step of the computational graph - the forward propagation (steps `1-5`) and the backward propagation / update (step `6`). To perform these steps, `xs` and `ys` are necessary, and are passed in via the `feed_dict` argument. The keys of the `feed_dict` are the variables referring to the placeholder objects in the graph, and the values are the data that will be used for those placeholders. After running `train`, the `betas` will no longer correspond to their original values. Note that in the code above we also ask for `E` back, which is what allows us to track our mean squared error through each iteration. \n",
    "\n",
    "Finally, if we run `train` through `Session.run` in a loop (as we do), we see that we can solve our multiple linear regression using this graph built in `tensorflow` and obtain the coefficient values that we expect.  \n",
    "\n",
    "Now, we'll move on to coding this up with `keras`. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
